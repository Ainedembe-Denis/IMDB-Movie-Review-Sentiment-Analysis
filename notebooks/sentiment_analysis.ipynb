{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Review Sentiment Analysis with EDA, Feature Engineering, and ML\n",
    "\n",
    "This notebook implements a comprehensive sentiment analysis pipeline covering:\n",
    "- Data Loading & Preprocessing, First EDA, Feature Engineering, Second EDA & Statistical Inference, Machine Learning, Presentation & Reflection\n",
    "\n",
    "### Dataset\n",
    "- **Source**: IMDB Movie Reviews Dataset - https://www.kaggle.com/datasets/mahmoudshaheen1134/imdp-data\n",
    "- **Location**: `../dataset/IMDB-Dataset.csv`\n",
    "### Authors\n",
    "- **Students**: Ainedembe Denis, Musinguzi Benson\n",
    "- **Lecturer**: Harriet Sibitenda (PhD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MANIPULATION AND ANALYSIS\n",
    "import pandas as pd  # Data manipulation and analysis: DataFrames, data loading, cleaning\n",
    "import numpy as np   # Numerical computing: arrays, mathematical operations, random number generation\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt   # Plotting library: create charts, graphs, histograms\n",
    "import seaborn as sns             # Statistical visualization: enhanced plots, heatmaps, statistical graphics\n",
    "from wordcloud import WordCloud   # Word cloud generation: visualize most frequent words in text\n",
    "\n",
    "# MACHINE LEARNING - MODEL SELECTION AND TRAINING\n",
    "from sklearn.model_selection import train_test_split  # Split data into training and testing sets\n",
    "from sklearn.model_selection import KFold             # K-fold cross-validation iterator\n",
    "\n",
    "# MACHINE LEARNING - FEATURE EXTRACTION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text to TF-IDF features (term frequency-inverse document frequency)\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convert text to bag-of-words count features\n",
    "from collections import Counter                              # Count occurrences of elements in iterable i.e used for word frequency\n",
    "\n",
    "# MACHINE LEARNING - CLASSIFICATION MODELS\n",
    "from sklearn.naive_bayes import MultinomialNB               # Naive Bayes classifier for text classification\n",
    "from sklearn.linear_model import LogisticRegression         # Logistic regression classifier\n",
    "from sklearn.svm import LinearSVC                           # Linear Support Vector Machine classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier     # Gradient Boosting ensemble classifier\n",
    "\n",
    "# MACHINE LEARNING - MODEL EVALUATION METRICS\n",
    "from sklearn.metrics import accuracy_score      # Calculate classification accuracy\n",
    "from sklearn.metrics import precision_score     # Calculate precision - positive predictive value\n",
    "from sklearn.metrics import recall_score        # Calculate recall (sensitivity, true positive rate)\n",
    "from sklearn.metrics import f1_score            # Calculate F1 score (harmonic mean of precision and recall)\n",
    "from sklearn.metrics import confusion_matrix    # Generate confusion matrix for classification results\n",
    "from sklearn.metrics import roc_auc_score       # Calculate ROC-AUC (Area Under ROC Curve)\n",
    "\n",
    "# MACHINE LEARNING - UTILITIES\n",
    "from sklearn.utils import resample                      # Resample datasets for balancing classes\n",
    "from sklearn.preprocessing import MinMaxScaler          # Scale features to a specific range (0-1)\n",
    "from sklearn.base import clone                          # Clone/copy machine learning models\n",
    "from sklearn.calibration import CalibratedClassifierCV  # Calibrate classifier probabilities\n",
    "\n",
    "# STATISTICAL ANALYSIS\n",
    "from scipy import stats               # Statistical functions: distributions, statistical tests (stats.sem, stats.t.ppf)\n",
    "from scipy.stats import ttest_ind     # Independent samples t-test for hypothesis testing\n",
    "\n",
    "# NATURAL LANGUAGE PROCESSING\n",
    "import nltk                                    # Natural Language Toolkit: NLP library\n",
    "from nltk.corpus import stopwords              # Common stopwords (e.g., \"the\", \"a\", \"is\") to filter out\n",
    "import re                                      # Regular expressions: pattern matching for text cleaning (HTML removal, punctuation)\n",
    "\n",
    "# NLTK DATA DOWNLOAD AND INITIALIZATION\n",
    "# Download stopwords corpus\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Initialize stopwords set for English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# Set random seed for reproducibility (ensures consistent results across runs)\n",
    "np.random.seed(42)\n",
    "\n",
    "# DISPLAY SETTINGS\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)      # Show all columns when printing DataFrames\n",
    "pd.set_option('display.max_colwidth', 100)      # Maximum column width when displaying text\n",
    "\n",
    "# Configure matplotlib and seaborn visualization styles\n",
    "plt.style.use('seaborn-v0_8')                   # Use seaborn style for plots\n",
    "sns.set_palette(\"husl\")                         # Set color palette for seaborn plots\n",
    "# Display plots inline in Jupyter notebook\n",
    "%matplotlib inline                              \n",
    "\n",
    "print(\"Libraries imported and environment set up successfully.\")\n",
    "print(f\"NLTK data ready. Loaded {len(stop_words)} stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Data Loading & Preprocessing\n",
    "\n",
    "#### A1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV file\n",
    "# The dataset contains movie reviews and their sentiment labels\n",
    "df = pd.read_csv('../dataset/IMDB-Dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()  # Display first few rows to inspect the data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "msing_vl = df.isnull().sum().sum()\n",
    "print(f\"Missing values: {msing_vl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Text Cleaning\n",
    "\n",
    "Removing HTML tags, Converting to lowercase, Removing punctuation & Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample review before cleaning\n",
    "print(\"Sample review of first 300 characters:\")\n",
    "print(df['review'].iloc[0][:300])\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "# This function implements the required preprocessing steps:\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle missing or empty text\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    # Step 1: Remove HTML tags using regex pattern matching\n",
    "    text = re.sub(r'<[^>]+>', '', str(text))\n",
    "    # Step 2: Convert to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    # Step 3: Remove punctuation - keep alphanumeric and spaces only\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # Step 4: Simple tokenization - split on whitespace\n",
    "    tokens = text.split()\n",
    "    # Step 5: Remove stopwords - common words like \"the\", \"a\", \"is\" that don't carry sentiment and \n",
    "    # - short words less than 2 characters to filter out noise\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    # Join tokens back to string for vectorization\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test cleaning function\n",
    "sample_text = df['review'].iloc[0]\n",
    "cleaned_sample = clean_text(sample_text)\n",
    "print(\"Original df with first 200 chars:\", sample_text[:300])\n",
    "print(\"Cleaned df with first 200 chars:\", cleaned_sample[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all reviews\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "print(\"Apply cleaning to all reviews completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical features using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "# We have chosen TF-IDF has over Bag-of-Words as it weights words by importance. rare words get higher scores\n",
    "# Parameters:\n",
    "#   - max_features=5000: Limit vocabulary to top 5000 features - this reduces dimensionality\n",
    "#   - ngram_range=(1, 2): Include unigrams (single words) and bigrams (word pairs) for context\n",
    "#   - min_df=2: Ignore terms appearing in less than 2 documents (filter rare terms)\n",
    "#   - max_df=0.95: Ignore terms appearing in more than 95% of documents (filter common terms)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_review'])  # Transform text to TF-IDF matrix\n",
    "y = df['sentiment'].map({'negative': 0, 'positive': 1})  # Convert labels to binary (0=negative, 1=positive)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Handling class balance if dataset is imbalanced\n",
    "\n",
    "Check sentiment distribution, Check if balanced, Visualize sentiment distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(f\"\\nSentiment distribution:\\n{sentiment_counts}\")\n",
    "\n",
    "percg = df['sentiment'].value_counts(normalize=True) * 100\n",
    "print(f\"\\nPercentage:\\n{percg}\")\n",
    "\n",
    "# Check if balanced\n",
    "is_balanced = df['sentiment'].value_counts(normalize=True).std() < 0.05\n",
    "print(f\"\\nDataset is {'balanced' if is_balanced else 'imbalanced'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'red'], edgecolor='black')\n",
    "plt.title('Sentiment Distribution in IMDB Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(sentiment_counts):\n",
    "    plt.text(i, v + 500, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: First Exploratory Data Analysis (EDA)\n",
    "\n",
    "B1. Most Frequent Words by Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words for positive and negative reviews\n",
    "# Part B requirement: \"Compute most frequent words in positive vs negative reviews\"\n",
    "\n",
    "def get_top_words(reviews, n=20):\n",
    "    #Get top N words from reviews by frequency count.\n",
    "    all_words = []\n",
    "    # Collect all words from all reviews in the set\n",
    "    for review in reviews:\n",
    "        all_words.extend(review.split())  # Split each review into words and add to list\n",
    "    # Count word frequencies and return top N\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "# Positive reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['cleaned_review']\n",
    "positive_words = get_top_words(positive_reviews, 20)\n",
    "\n",
    "# Negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['cleaned_review']\n",
    "negative_words = get_top_words(negative_reviews, 20)\n",
    "\n",
    "print(\"Top 10 words in POSITIVE reviews:\")\n",
    "for word, count in positive_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in NEGATIVE reviews:\")\n",
    "for word, count in negative_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Word Clouds for Each Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Positive reviews word cloud\n",
    "positive_text = ' '.join(positive_reviews)\n",
    "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "axes[0].imshow(wordcloud_positive, interpolation='bilinear')\n",
    "axes[0].set_title('Positive Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative reviews word cloud\n",
    "negative_text = ' '.join(negative_reviews)\n",
    "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
    "axes[1].imshow(wordcloud_negative, interpolation='bilinear')\n",
    "axes[1].set_title('Negative Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Histogram of Review Lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review lengths\n",
    "df['review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Create histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram by sentiment\n",
    "positive_lengths = df[df['sentiment'] == 'positive']['review_length']\n",
    "negative_lengths = df[df['sentiment'] == 'negative']['review_length']\n",
    "\n",
    "axes[0].hist([positive_lengths, negative_lengths], bins=50, alpha=0.7, \n",
    "             label=['Positive', 'Negative'], color=['green', 'red'])\n",
    "axes[0].set_xlabel('Review Length (words)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Review Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([positive_lengths, negative_lengths], labels=['Positive', 'Negative'])\n",
    "axes[1].set_ylabel('Review Length (words)', fontsize=12)\n",
    "axes[1].set_title('Review Length Box Plot by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Positive: Mean={positive_lengths.mean():.1f}, Median={positive_lengths.median():.1f}, Std={positive_lengths.std():.1f}\")\n",
    "print(f\"Negative: Mean={negative_lengths.mean():.1f}, Median={negative_lengths.median():.1f}, Std={negative_lengths.std():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. Do Positive vs Negative Reviews Differ in Length or Vocabulary?\n",
    "\n",
    "**Findings:**\n",
    "1. **Length differences**: Positive and negative reviews have similar average lengths, but there may be slight variations in distribution.\n",
    "2. **Vocabulary differences**: Positive reviews contain words like \"great\", \"excellent\", \"wonderful\", \"love\", \"good\", \"best\", \"amazing\", \"enjoy\", \"perfect\", \"brilliant\".\n",
    "3. **Negative reviews** contain words like \"bad\", \"worst\", \"awful\", \"terrible\", \"horrible\", \"waste\", \"boring\", \"disappointing\", \"poor\", \"dull\".\n",
    "4. **Key insight**: While length may be similar, vocabulary clearly differs between positive and negative reviews, which makes sentiment classification feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Feature Engineering\n",
    "\n",
    "C1. Create sentiment lexicon score per review (positive − negative word counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT LEXICON SCORE - TWO APPROACHES,  MANUAL LEXICON & DYNAMIC EXTRACTION\n",
    "# (Choose ONE approach by commenting/uncommenting the appropriate sections below)\n",
    "# ============================================================================\n",
    "# \n",
    "# OPTION 1: MANUAL LEXICON\n",
    "#   - Pros: Fast, interpretable, based on linguistic knowledge\n",
    "#   - Cons: Limited coverage, may miss domain-specific terms\n",
    "#\n",
    "# OPTION 2: DYNAMIC EXTRACTION (Advanced - Data-Driven)\n",
    "#   - Pros: Discovers domain-specific words automatically, data-driven\n",
    "#   - Cons: More complex, may include noise, requires data to be loaded first\n",
    "# ============================================================================\n",
    "\n",
    "# OPTION 1: MANUAL LEXICON (Pre-defined word lists)\n",
    "# Comment out this entire section if using OPTION 2\n",
    "\n",
    "positive_words_lexicon = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', \n",
    "                          'love', 'like', 'best', 'perfect', 'brilliant', 'awesome', 'enjoy',\n",
    "                          'beautiful', 'outstanding', 'superb', 'marvelous', 'fabulous', 'delightful']\n",
    "\n",
    "negative_words_lexicon = ['bad', 'worst', 'awful', 'terrible', 'horrible', 'hate', 'disappointing',\n",
    "                          'poor', 'boring', 'dull', 'waste', 'stupid', 'annoying', 'frustrating',\n",
    "                          'disgusting', 'pathetic', 'ridiculous', 'unpleasant', 'ugly']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# OPTION 2: DYNAMIC EXTRACTION (Extract from dataset)\n",
    "# Uncomment this entire section if using OPTION 2 (and comment out OPTION 1 above)\n",
    "\n",
    "# Get word frequencies for positive and negative reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['cleaned_review']\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['cleaned_review']\n",
    "\n",
    "# Count words in each class\n",
    "positive_word_counts = Counter()\n",
    "negative_word_counts = Counter()\n",
    "\n",
    "for review in positive_reviews:\n",
    "    positive_word_counts.update(review.split())\n",
    "\n",
    "for review in negative_reviews:\n",
    "    negative_word_counts.update(review.split())\n",
    "\n",
    "# Calculate relative frequency (word frequency / total words in class)\n",
    "total_positive_words = sum(positive_word_counts.values())\n",
    "total_negative_words = sum(negative_word_counts.values())\n",
    "\n",
    "# Get all unique words\n",
    "all_words = set(positive_word_counts.keys()) | set(negative_word_counts.keys())\n",
    "\n",
    "# Calculate sentiment score for each word\n",
    "# Score = (freq in positive / total positive) - (freq in negative / total negative)\n",
    "word_sentiment_scores = {}\n",
    "for word in all_words:\n",
    "    pos_freq = positive_word_counts.get(word, 0) / total_positive_words if total_positive_words > 0 else 0\n",
    "    neg_freq = negative_word_counts.get(word, 0) / total_negative_words if total_negative_words > 0 else 0\n",
    "    word_sentiment_scores[word] = pos_freq - neg_freq\n",
    "\n",
    "# Extract top positive and negative words (words with highest/lowest scores)\n",
    "# Filter by minimum frequency to avoid rare words\n",
    "min_frequency = 10  # Word must appear at least 10 times\n",
    "positive_words_dynamic = [word for word, score in sorted(word_sentiment_scores.items(), \n",
    "                                                          key=lambda x: x[1], reverse=True)\n",
    "                          if positive_word_counts.get(word, 0) >= min_frequency][:30]\n",
    "\n",
    "negative_words_dynamic = [word for word, score in sorted(word_sentiment_scores.items(), \n",
    "                                                          key=lambda x: x[1])\n",
    "                          if negative_word_counts.get(word, 0) >= min_frequency][:30]\n",
    "\n",
    "print(\"Top 20 Dynamically Extracted +ve Words:\", positive_words_dynamic[:20])\n",
    "print(\"Top 20 Dynamically Extracted -ve Words:\", negative_words_dynamic[:20])\n",
    "\n",
    "# Use dynamically extracted lexicons\n",
    "positive_words_lexicon = positive_words_dynamic\n",
    "negative_words_lexicon = negative_words_dynamic\n",
    "\"\"\"\n",
    "\n",
    "#Calculate sentiment lexicon score (positive - negative word counts).\n",
    "def calculate_lexicon_score(text):\n",
    "    \n",
    "    # Returns a score where:\n",
    "    #- Positive values indicate more positive words ; Negative values indicate more negative words\n",
    "    #- Zero indicates balanced or neutral\n",
    "   \n",
    "    words = text.lower().split()  # Convert to lowercase and split into words\n",
    "    # Count positive words in the text\n",
    "    positive_count = sum(1 for word in words if word in positive_words_lexicon)\n",
    "    # Count negative words in the text\n",
    "    negative_count = sum(1 for word in words if word in negative_words_lexicon)\n",
    "    # Return difference (positive - negative)\n",
    "    return positive_count - negative_count\n",
    "\n",
    "# Calculate lexicon scores\n",
    "df['lexicon_score'] = df['cleaned_review'].apply(calculate_lexicon_score)\n",
    "\n",
    "print(\"Lexicon score statistics:\")\n",
    "print(df['lexicon_score'].describe())\n",
    "print(f\"\\nMean by sentiment:\\n{df.groupby('sentiment')['lexicon_score'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Extract N-grams (Bigrams and Trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bigrams and trigrams\n",
    "# N-grams capture context and phrases that single words miss (e.g., \"not good\" vs \"good\")\n",
    "# Extract bigrams (2-word phrases)\n",
    "# Example: \"very good\", \"not bad\", \"really great\"\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=20)\n",
    "bigrams = bigram_vectorizer.fit_transform(df['cleaned_review'])\n",
    "bigram_features = bigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract trigrams (3-word phrases)\n",
    "# Example: \"not very good\", \"one of the\"\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=20)\n",
    "trigrams = trigram_vectorizer.fit_transform(df['cleaned_review'])\n",
    "trigram_features = trigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top 10 Bigrams:\", list(bigram_features[:10]))\n",
    "print(\"Top 10 Trigrams:\", list(trigram_features[:10]))\n",
    "\n",
    "# Store n-gram counts as features\n",
    "df['bigram_count'] = [bigrams[i].sum() for i in range(len(df))]\n",
    "df['trigram_count'] = [trigrams[i].sum() for i in range(len(df))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3. Compute Readability Metrics\n",
    "Compute readability metrics (e.g., average word length, sentence length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #These metrics capture writing style and complexity, which may correlate with sentiment expression\n",
    "    #Compute readability metrics: average word length, sentence length.\n",
    "    \n",
    "    #Returns: avg_sentence_length: Average number of words per sentence; avg_sentence_length: Average number of words per sentence\n",
    "def compute_readability_metrics(text):\n",
    "    # Calculate average word length the indicator of vocabulary complexity\n",
    "    words = text.split()\n",
    "    if len(words) > 0:\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        avg_word_length = 0\n",
    "    \n",
    "    # Calculate average sentence length the indicator of writing complexity\n",
    "    # Approximate sentence count by counting sentence-ending punctuation\n",
    "    # Note: This is an approximation since cleaned text may have removed some punctuation\n",
    "    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n",
    "    avg_sentence_length = len(words) / sentence_count if sentence_count > 0 else len(words)\n",
    "    \n",
    "    return avg_word_length, avg_sentence_length\n",
    "\n",
    "# Apply readability metrics\n",
    "readability_metrics = df['cleaned_review'].apply(compute_readability_metrics)\n",
    "df['avg_word_length'] = [m[0] for m in readability_metrics]\n",
    "df['avg_sentence_length'] = [m[1] for m in readability_metrics]\n",
    "\n",
    "print(\"Readability metrics statistics:\")\n",
    "print(df[['avg_word_length', 'avg_sentence_length']].describe())\n",
    "print(f\"\\nMean by sentiment:\\n{df.groupby('sentiment')[['avg_word_length', 'avg_sentence_length']].mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4. Why These Engineered Features Help Sentiment Classification\n",
    "\n",
    "Importance of these Features:\n",
    "1. *Sentiment Lexicon Score*: Directly captures sentiment polarity by counting positive vs negative words. Higher scores indicate positive sentiment. Lexicon scores provide direct sentiment signals\n",
    "2. *N-grams (Bigrams/Trigrams)*: Capture context and phrases that single words miss (e.g., \"not good\" vs \"good\"). It handles negations better than unigrams\n",
    "3. *Readability Metrics*: Captures stylistic differences that may correlate with sentiment\n",
    "   Average word length: Longer words may indicate more formal or detailed reviews\n",
    "   Average sentence length: Can indicate review complexity and writing style\n",
    "   These metrics may correlate with review quality and sentiment expression\n",
    "\n",
    "Combining these with TF-IDF features provides richer feature representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second EDA & Statistical Inference\n",
    "\n",
    "D1. Hypothesis Test: Are Reviews with Higher Lexicon Scores More Likely to be Positive?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis test: Are reviews with higher lexicon scores more likely to be positive?\n",
    "# Hypothesis test: Are reviews with higher lexicon scores more likely to be positive?\"\n",
    "# \n",
    "# Null Hypothesis (H0): No difference in lexicon scores between positive and negative reviews\n",
    "# Alternative Hypothesis (H1): Positive reviews have higher lexicon scores than negative reviews\n",
    "# Significance level: α = 0.05\n",
    "\n",
    "# Extract lexicon scores for each sentiment class\n",
    "positive_scores = df[df['sentiment'] == 'positive']['lexicon_score']\n",
    "negative_scores = df[df['sentiment'] == 'negative']['lexicon_score']\n",
    "\n",
    "# Perform independent samples t-test (one-tailed, right-tailed)\n",
    "# Tests if positive reviews have significantly higher lexicon scores\n",
    "t_stat, p_value = ttest_ind(positive_scores, negative_scores, alternative='greater')\n",
    "\n",
    "print(f\"H0: No difference in lexicon scores\")\n",
    "print(f\"H1: Positive reviews have higher lexicon scores\")\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}\")\n",
    "print(f\"Conclusion: {'Reject H0' if p_value < 0.05 else 'Fail to reject H0'} (α = 0.05)\")\n",
    "print(f\"Reviews with higher lexicon scores are {'more likely' if p_value < 0.05 else 'not significantly more likely'} to be positive.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2. Confidence Interval for Average Review Length by Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for average review length\n",
    "# Confidence intervals provide a range estimate for the true population mean\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    \n",
    "    Calculate confidence interval for mean using t-distribution.\n",
    "    \n",
    "    Args: 'data: Array-like data'; 'confidence: Confidence level (default 0.95 for 95% CI)'\n",
    "    \n",
    "    Returns: Tuple of (lower_bound, upper_bound)\n",
    "    n = len(data)\n",
    "    mean = data.mean()  # Sample mean\n",
    "    std_err = stats.sem(data)  # Standard error of the mean\n",
    "    # Calculate margin of error using t-distribution (for sample size < 30 or unknown population std)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean - h, mean + h  # Return (lower, upper) bounds\n",
    "\n",
    "# Positive reviews\n",
    "pos_mean = positive_lengths.mean()\n",
    "pos_ci = confidence_interval(positive_lengths)\n",
    "\n",
    "# Negative reviews\n",
    "neg_mean = negative_lengths.mean()\n",
    "neg_ci = confidence_interval(negative_lengths)\n",
    "\n",
    "print(\"95% Confidence Intervals for Average Review Length:\")\n",
    "print(f\"Positive: Mean={pos_mean:.2f}, CI=[{pos_ci[0]:.2f}, {pos_ci[1]:.2f}]\")\n",
    "print(f\"Negative: Mean={neg_mean:.2f}, CI=[{neg_ci[0]:.2f}, {neg_ci[1]:.2f}]\")\n",
    "print(f\"Difference: {pos_mean - neg_mean:.2f} words, CI overlap: {'Yes' if pos_ci[1] >= neg_ci[0] and neg_ci[1] >= pos_ci[0] else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3. Visualize Sentiment Differences in Engineered Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize engineered features by sentiment\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Lexicon score distribution\n",
    "axes[0, 0].hist([df[df['sentiment'] == 'positive']['lexicon_score'],\n",
    "                 df[df['sentiment'] == 'negative']['lexicon_score']],\n",
    "                bins=30, alpha=0.7, label=['Positive', 'Negative'], color=['green', 'red'])\n",
    "axes[0, 0].set_xlabel('Lexicon Score', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Lexicon Score Distribution by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Average word length\n",
    "axes[0, 1].boxplot([df[df['sentiment'] == 'positive']['avg_word_length'],\n",
    "                    df[df['sentiment'] == 'negative']['avg_word_length']],\n",
    "                   labels=['Positive', 'Negative'])\n",
    "axes[0, 1].set_ylabel('Average Word Length', fontsize=12)\n",
    "axes[0, 1].set_title('Average Word Length by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Average sentence length\n",
    "axes[1, 0].boxplot([df[df['sentiment'] == 'positive']['avg_sentence_length'],\n",
    "                    df[df['sentiment'] == 'negative']['avg_sentence_length']],\n",
    "                   labels=['Positive', 'Negative'])\n",
    "axes[1, 0].set_ylabel('Average Sentence Length', fontsize=12)\n",
    "axes[1, 0].set_title('Average Sentence Length by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Review length\n",
    "axes[1, 1].boxplot([positive_lengths, negative_lengths], labels=['Positive', 'Negative'])\n",
    "axes[1, 1].set_ylabel('Review Length (words)', fontsize=12)\n",
    "axes[1, 1].set_title('Review Length by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D4. Interpret Findings in Context\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Lexicon Score**: Positive reviews have significantly higher lexicon scores than negative reviews as confirmed by hypothesis test. This validates that lexicon-based features are effective for sentiment classification.\n",
    "2. **Review Length**: Positive and negative reviews have similar average lengths, with overlapping confidence intervals. Length alone is not a strong predictor of sentiment.\n",
    "3. **Readability Metrics**: Average word length and sentence length show similar distributions for both sentiments, suggesting that writing style (complexity) is not strongly correlated with sentiment.\n",
    "4. **Implications**: \n",
    "   - Lexicon-based features are the most discriminative engineered features\n",
    "   - TF-IDF features capturing vocabulary differences are likely more important than readability metrics\n",
    "   - Combining lexicon scores with TF-IDF features should improve classification performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
