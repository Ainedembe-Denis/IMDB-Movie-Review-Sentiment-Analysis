{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Review Sentiment Analysis with EDA, Feature Engineering, and ML\n",
    "\n",
    "This notebook implements a comprehensive sentiment analysis pipeline covering:\n",
    "- Data Loading & Preprocessing, First EDA, Feature Engineering, Second EDA & Statistical Inference, Machine Learning, Presentation & Reflection\n",
    "\n",
    "### Dataset\n",
    "- **Source**: IMDB Movie Reviews Dataset - https://www.kaggle.com/datasets/mahmoudshaheen1134/imdp-data\n",
    "- **Location**: `../dataset/IMDB-Dataset.csv`\n",
    "### Authors\n",
    "- **Students**: Ainedembe Denis, Musinguzi Benson\n",
    "- **Lecturer**: Harriet Sibitenda (PhD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MANIPULATION AND ANALYSIS\n",
    "import pandas as pd  # Data manipulation and analysis: DataFrames, data loading, cleaning\n",
    "import numpy as np   # Numerical computing: arrays, mathematical operations, random number generation\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt   # Plotting library: create charts, graphs, histograms\n",
    "import seaborn as sns             # Statistical visualization: enhanced plots, heatmaps, statistical graphics\n",
    "from wordcloud import WordCloud   # Word cloud generation: visualize most frequent words in text\n",
    "\n",
    "# MACHINE LEARNING - MODEL SELECTION AND TRAINING\n",
    "from sklearn.model_selection import train_test_split  # Split data into training and testing sets\n",
    "from sklearn.model_selection import KFold             # K-fold cross-validation iterator\n",
    "\n",
    "# MACHINE LEARNING - FEATURE EXTRACTION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text to TF-IDF features (term frequency-inverse document frequency)\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convert text to bag-of-words count features\n",
    "from collections import Counter                              # Count occurrences of elements in iterable i.e used for word frequency\n",
    "\n",
    "# MACHINE LEARNING - CLASSIFICATION MODELS\n",
    "from sklearn.naive_bayes import MultinomialNB               # Naive Bayes classifier for text classification\n",
    "from sklearn.linear_model import LogisticRegression         # Logistic regression classifier\n",
    "from sklearn.svm import LinearSVC                           # Linear Support Vector Machine classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier     # Gradient Boosting ensemble classifier\n",
    "\n",
    "# MACHINE LEARNING - MODEL EVALUATION METRICS\n",
    "from sklearn.metrics import accuracy_score      # Calculate classification accuracy\n",
    "from sklearn.metrics import precision_score     # Calculate precision - positive predictive value\n",
    "from sklearn.metrics import recall_score        # Calculate recall (sensitivity, true positive rate)\n",
    "from sklearn.metrics import f1_score            # Calculate F1 score (harmonic mean of precision and recall)\n",
    "from sklearn.metrics import confusion_matrix    # Generate confusion matrix for classification results\n",
    "from sklearn.metrics import roc_auc_score       # Calculate ROC-AUC (Area Under ROC Curve)\n",
    "\n",
    "# MACHINE LEARNING - UTILITIES\n",
    "from sklearn.utils import resample                      # Resample datasets for balancing classes\n",
    "from sklearn.preprocessing import MinMaxScaler          # Scale features to a specific range (0-1)\n",
    "from sklearn.base import clone                          # Clone/copy machine learning models\n",
    "from sklearn.calibration import CalibratedClassifierCV  # Calibrate classifier probabilities\n",
    "\n",
    "# STATISTICAL ANALYSIS\n",
    "from scipy import stats               # Statistical functions: distributions, statistical tests (stats.sem, stats.t.ppf)\n",
    "from scipy.stats import ttest_ind     # Independent samples t-test for hypothesis testing\n",
    "\n",
    "# NATURAL LANGUAGE PROCESSING\n",
    "import nltk                                    # Natural Language Toolkit: NLP library\n",
    "from nltk.corpus import stopwords              # Common stopwords (e.g., \"the\", \"a\", \"is\") to filter out\n",
    "import re                                      # Regular expressions: pattern matching for text cleaning (HTML removal, punctuation)\n",
    "\n",
    "# NLTK DATA DOWNLOAD AND INITIALIZATION\n",
    "# Download stopwords corpus\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Initialize stopwords set for English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# Set random seed for reproducibility (ensures consistent results across runs)\n",
    "np.random.seed(42)\n",
    "\n",
    "# DISPLAY SETTINGS\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)      # Show all columns when printing DataFrames\n",
    "pd.set_option('display.max_colwidth', 100)      # Maximum column width when displaying text\n",
    "\n",
    "# Configure matplotlib and seaborn visualization styles\n",
    "plt.style.use('seaborn-v0_8')                   # Use seaborn style for plots\n",
    "sns.set_palette(\"husl\")                         # Set color palette for seaborn plots\n",
    "# Display plots inline in Jupyter notebook\n",
    "%matplotlib inline                              \n",
    "\n",
    "print(\"Libraries imported and environment set up successfully.\")\n",
    "print(f\"NLTK data ready. Loaded {len(stop_words)} stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Data Loading & Preprocessing\n",
    "\n",
    "#### A1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV file\n",
    "# The dataset contains movie reviews and their sentiment labels\n",
    "df = pd.read_csv('../dataset/IMDB-Dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()  # Display first few rows to inspect the data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "msing_vl = df.isnull().sum().sum()\n",
    "print(f\"Missing values: {msing_vl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Text Cleaning\n",
    "\n",
    "Removing HTML tags, Converting to lowercase, Removing punctuation & Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample review before cleaning\n",
    "print(\"Sample review of first 300 characters:\")\n",
    "print(df['review'].iloc[0][:300])\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "# This function implements the required preprocessing steps:\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle missing or empty text\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    # Step 1: Remove HTML tags using regex pattern matching\n",
    "    text = re.sub(r'<[^>]+>', '', str(text))\n",
    "    # Step 2: Convert to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    # Step 3: Remove punctuation - keep alphanumeric and spaces only\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # Step 4: Simple tokenization - split on whitespace\n",
    "    tokens = text.split()\n",
    "    # Step 5: Remove stopwords - common words like \"the\", \"a\", \"is\" that don't carry sentiment and \n",
    "    # - short words less than 2 characters to filter out noise\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    # Join tokens back to string for vectorization\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test cleaning function\n",
    "sample_text = df['review'].iloc[0]\n",
    "cleaned_sample = clean_text(sample_text)\n",
    "print(\"Original df with first 200 chars:\", sample_text[:300])\n",
    "print(\"Cleaned df with first 200 chars:\", cleaned_sample[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all reviews\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "print(\"Apply cleaning to all reviews completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical features using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "# We have chosen TF-IDF has over Bag-of-Words as it weights words by importance. rare words get higher scores\n",
    "# Parameters:\n",
    "#   - max_features=5000: Limit vocabulary to top 5000 features - this reduces dimensionality\n",
    "#   - ngram_range=(1, 2): Include unigrams (single words) and bigrams (word pairs) for context\n",
    "#   - min_df=2: Ignore terms appearing in less than 2 documents (filter rare terms)\n",
    "#   - max_df=0.95: Ignore terms appearing in more than 95% of documents (filter common terms)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_review'])  # Transform text to TF-IDF matrix\n",
    "y = df['sentiment'].map({'negative': 0, 'positive': 1})  # Convert labels to binary (0=negative, 1=positive)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Handling class balance if dataset is imbalanced\n",
    "\n",
    "Check sentiment distribution, Check if balanced, Visualize sentiment distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(f\"\\nSentiment distribution:\\n{sentiment_counts}\")\n",
    "\n",
    "percg = df['sentiment'].value_counts(normalize=True) * 100\n",
    "print(f\"\\nPercentage:\\n{percg}\")\n",
    "\n",
    "# Check if balanced\n",
    "is_balanced = df['sentiment'].value_counts(normalize=True).std() < 0.05\n",
    "print(f\"\\nDataset is {'balanced' if is_balanced else 'imbalanced'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'red'], edgecolor='black')\n",
    "plt.title('Sentiment Distribution in IMDB Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(sentiment_counts):\n",
    "    plt.text(i, v + 500, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: First Exploratory Data Analysis (EDA)\n",
    "\n",
    "B1. Most Frequent Words by Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words for positive and negative reviews\n",
    "# Part B requirement: \"Compute most frequent words in positive vs negative reviews\"\n",
    "\n",
    "def get_top_words(reviews, n=20):\n",
    "    #Get top N words from reviews by frequency count.\n",
    "    all_words = []\n",
    "    # Collect all words from all reviews in the set\n",
    "    for review in reviews:\n",
    "        all_words.extend(review.split())  # Split each review into words and add to list\n",
    "    # Count word frequencies and return top N\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "# Positive reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['cleaned_review']\n",
    "positive_words = get_top_words(positive_reviews, 20)\n",
    "\n",
    "# Negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['cleaned_review']\n",
    "negative_words = get_top_words(negative_reviews, 20)\n",
    "\n",
    "print(\"Top 10 words in POSITIVE reviews:\")\n",
    "for word, count in positive_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in NEGATIVE reviews:\")\n",
    "for word, count in negative_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Word Clouds for Each Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Positive reviews word cloud\n",
    "positive_text = ' '.join(positive_reviews)\n",
    "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "axes[0].imshow(wordcloud_positive, interpolation='bilinear')\n",
    "axes[0].set_title('Positive Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative reviews word cloud\n",
    "negative_text = ' '.join(negative_reviews)\n",
    "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
    "axes[1].imshow(wordcloud_negative, interpolation='bilinear')\n",
    "axes[1].set_title('Negative Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Histogram of Review Lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review lengths\n",
    "df['review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Create histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram by sentiment\n",
    "positive_lengths = df[df['sentiment'] == 'positive']['review_length']\n",
    "negative_lengths = df[df['sentiment'] == 'negative']['review_length']\n",
    "\n",
    "axes[0].hist([positive_lengths, negative_lengths], bins=50, alpha=0.7, \n",
    "             label=['Positive', 'Negative'], color=['green', 'red'])\n",
    "axes[0].set_xlabel('Review Length (words)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Review Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([positive_lengths, negative_lengths], labels=['Positive', 'Negative'])\n",
    "axes[1].set_ylabel('Review Length (words)', fontsize=12)\n",
    "axes[1].set_title('Review Length Box Plot by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Positive: Mean={positive_lengths.mean():.1f}, Median={positive_lengths.median():.1f}, Std={positive_lengths.std():.1f}\")\n",
    "print(f\"Negative: Mean={negative_lengths.mean():.1f}, Median={negative_lengths.median():.1f}, Std={negative_lengths.std():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. Do Positive vs Negative Reviews Differ in Length or Vocabulary?\n",
    "\n",
    "**Findings:**\n",
    "1. **Length differences**: Positive and negative reviews have similar average lengths, but there may be slight variations in distribution.\n",
    "2. **Vocabulary differences**: Positive reviews contain words like \"great\", \"excellent\", \"wonderful\", \"love\", \"good\", \"best\", \"amazing\", \"enjoy\", \"perfect\", \"brilliant\".\n",
    "3. **Negative reviews** contain words like \"bad\", \"worst\", \"awful\", \"terrible\", \"horrible\", \"waste\", \"boring\", \"disappointing\", \"poor\", \"dull\".\n",
    "4. **Key insight**: While length may be similar, vocabulary clearly differs between positive and negative reviews, which makes sentiment classification feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Feature Engineering\n",
    "\n",
    "C1. Create sentiment lexicon score per review (positive âˆ’ negative word counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT LEXICON SCORE - TWO APPROACHES,  MANUAL LEXICON & DYNAMIC EXTRACTION\n",
    "# (Choose ONE approach by commenting/uncommenting the appropriate sections below)\n",
    "# ============================================================================\n",
    "# \n",
    "# OPTION 1: MANUAL LEXICON\n",
    "#   - Pros: Fast, interpretable, based on linguistic knowledge\n",
    "#   - Cons: Limited coverage, may miss domain-specific terms\n",
    "#\n",
    "# OPTION 2: DYNAMIC EXTRACTION (Advanced - Data-Driven)\n",
    "#   - Pros: Discovers domain-specific words automatically, data-driven\n",
    "#   - Cons: More complex, may include noise, requires data to be loaded first\n",
    "# ============================================================================\n",
    "\n",
    "# OPTION 1: MANUAL LEXICON (Pre-defined word lists)\n",
    "# Comment out this entire section if using OPTION 2\n",
    "\n",
    "positive_words_lexicon = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', \n",
    "                          'love', 'like', 'best', 'perfect', 'brilliant', 'awesome', 'enjoy',\n",
    "                          'beautiful', 'outstanding', 'superb', 'marvelous', 'fabulous', 'delightful']\n",
    "\n",
    "negative_words_lexicon = ['bad', 'worst', 'awful', 'terrible', 'horrible', 'hate', 'disappointing',\n",
    "                          'poor', 'boring', 'dull', 'waste', 'stupid', 'annoying', 'frustrating',\n",
    "                          'disgusting', 'pathetic', 'ridiculous', 'unpleasant', 'ugly']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# OPTION 2: DYNAMIC EXTRACTION (Extract from dataset)\n",
    "# Uncomment this entire section if using OPTION 2 (and comment out OPTION 1 above)\n",
    "\n",
    "# Get word frequencies for positive and negative reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['cleaned_review']\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['cleaned_review']\n",
    "\n",
    "# Count words in each class\n",
    "positive_word_counts = Counter()\n",
    "negative_word_counts = Counter()\n",
    "\n",
    "for review in positive_reviews:\n",
    "    positive_word_counts.update(review.split())\n",
    "\n",
    "for review in negative_reviews:\n",
    "    negative_word_counts.update(review.split())\n",
    "\n",
    "# Calculate relative frequency (word frequency / total words in class)\n",
    "total_positive_words = sum(positive_word_counts.values())\n",
    "total_negative_words = sum(negative_word_counts.values())\n",
    "\n",
    "# Get all unique words\n",
    "all_words = set(positive_word_counts.keys()) | set(negative_word_counts.keys())\n",
    "\n",
    "# Calculate sentiment score for each word\n",
    "# Score = (freq in positive / total positive) - (freq in negative / total negative)\n",
    "word_sentiment_scores = {}\n",
    "for word in all_words:\n",
    "    pos_freq = positive_word_counts.get(word, 0) / total_positive_words if total_positive_words > 0 else 0\n",
    "    neg_freq = negative_word_counts.get(word, 0) / total_negative_words if total_negative_words > 0 else 0\n",
    "    word_sentiment_scores[word] = pos_freq - neg_freq\n",
    "\n",
    "# Extract top positive and negative words (words with highest/lowest scores)\n",
    "# Filter by minimum frequency to avoid rare words\n",
    "min_frequency = 10  # Word must appear at least 10 times\n",
    "positive_words_dynamic = [word for word, score in sorted(word_sentiment_scores.items(), \n",
    "                                                          key=lambda x: x[1], reverse=True)\n",
    "                          if positive_word_counts.get(word, 0) >= min_frequency][:30]\n",
    "\n",
    "negative_words_dynamic = [word for word, score in sorted(word_sentiment_scores.items(), \n",
    "                                                          key=lambda x: x[1])\n",
    "                          if negative_word_counts.get(word, 0) >= min_frequency][:30]\n",
    "\n",
    "print(\"Top 20 Dynamically Extracted +ve Words:\", positive_words_dynamic[:20])\n",
    "print(\"Top 20 Dynamically Extracted -ve Words:\", negative_words_dynamic[:20])\n",
    "\n",
    "# Use dynamically extracted lexicons\n",
    "positive_words_lexicon = positive_words_dynamic\n",
    "negative_words_lexicon = negative_words_dynamic\n",
    "\"\"\"\n",
    "\n",
    "#Calculate sentiment lexicon score (positive - negative word counts).\n",
    "def calculate_lexicon_score(text):\n",
    "    \n",
    "    # Returns a score where:\n",
    "    #- Positive values indicate more positive words ; Negative values indicate more negative words\n",
    "    #- Zero indicates balanced or neutral\n",
    "   \n",
    "    words = text.lower().split()  # Convert to lowercase and split into words\n",
    "    # Count positive words in the text\n",
    "    positive_count = sum(1 for word in words if word in positive_words_lexicon)\n",
    "    # Count negative words in the text\n",
    "    negative_count = sum(1 for word in words if word in negative_words_lexicon)\n",
    "    # Return difference (positive - negative)\n",
    "    return positive_count - negative_count\n",
    "\n",
    "# Calculate lexicon scores\n",
    "df['lexicon_score'] = df['cleaned_review'].apply(calculate_lexicon_score)\n",
    "\n",
    "print(\"Lexicon score statistics:\")\n",
    "print(df['lexicon_score'].describe())\n",
    "print(f\"\\nMean by sentiment:\\n{df.groupby('sentiment')['lexicon_score'].mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
