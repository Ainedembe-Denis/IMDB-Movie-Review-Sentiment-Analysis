{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Review Sentiment Analysis with EDA, Feature Engineering, and ML\n",
    "\n",
    "This notebook implements a comprehensive sentiment analysis pipeline covering:\n",
    "- Data Loading & Preprocessing, First EDA, Feature Engineering, Second EDA & Statistical Inference, Machine Learning, Presentation & Reflection\n",
    "\n",
    "### Dataset\n",
    "- **Source**: IMDB Movie Reviews Dataset - https://www.kaggle.com/datasets/mahmoudshaheen1134/imdp-data\n",
    "- **Location**: `../dataset/IMDB-Dataset.csv`\n",
    "### Authors\n",
    "- **Students**: Ainedembe Denis, Musinguzi Benson\n",
    "- **Lecturer**: Harriet Sibitenda (PhD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MANIPULATION AND ANALYSIS\n",
    "import pandas as pd  # Data manipulation and analysis: DataFrames, data loading, cleaning\n",
    "import numpy as np   # Numerical computing: arrays, mathematical operations, random number generation\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt   # Plotting library: create charts, graphs, histograms\n",
    "import seaborn as sns             # Statistical visualization: enhanced plots, heatmaps, statistical graphics\n",
    "from wordcloud import WordCloud   # Word cloud generation: visualize most frequent words in text\n",
    "\n",
    "# MACHINE LEARNING - MODEL SELECTION AND TRAINING\n",
    "from sklearn.model_selection import train_test_split  # Split data into training and testing sets\n",
    "from sklearn.model_selection import KFold             # K-fold cross-validation iterator\n",
    "\n",
    "# MACHINE LEARNING - FEATURE EXTRACTION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text to TF-IDF features (term frequency-inverse document frequency)\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convert text to bag-of-words count features\n",
    "from collections import Counter                              # Count occurrences of elements in iterable i.e used for word frequency\n",
    "\n",
    "# MACHINE LEARNING - CLASSIFICATION MODELS\n",
    "from sklearn.naive_bayes import MultinomialNB               # Naive Bayes classifier for text classification\n",
    "from sklearn.linear_model import LogisticRegression         # Logistic regression classifier\n",
    "from sklearn.svm import LinearSVC                           # Linear Support Vector Machine classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier     # Gradient Boosting ensemble classifier\n",
    "\n",
    "# MACHINE LEARNING - MODEL EVALUATION METRICS\n",
    "from sklearn.metrics import accuracy_score      # Calculate classification accuracy\n",
    "from sklearn.metrics import precision_score     # Calculate precision - positive predictive value\n",
    "from sklearn.metrics import recall_score        # Calculate recall (sensitivity, true positive rate)\n",
    "from sklearn.metrics import f1_score            # Calculate F1 score (harmonic mean of precision and recall)\n",
    "from sklearn.metrics import confusion_matrix    # Generate confusion matrix for classification results\n",
    "from sklearn.metrics import roc_auc_score       # Calculate ROC-AUC (Area Under ROC Curve)\n",
    "\n",
    "# MACHINE LEARNING - UTILITIES\n",
    "from sklearn.utils import resample                      # Resample datasets for balancing classes\n",
    "from sklearn.preprocessing import MinMaxScaler          # Scale features to a specific range (0-1)\n",
    "from sklearn.base import clone                          # Clone/copy machine learning models\n",
    "from sklearn.calibration import CalibratedClassifierCV  # Calibrate classifier probabilities\n",
    "\n",
    "# STATISTICAL ANALYSIS\n",
    "from scipy import stats               # Statistical functions: distributions, statistical tests (stats.sem, stats.t.ppf)\n",
    "from scipy.stats import ttest_ind     # Independent samples t-test for hypothesis testing\n",
    "\n",
    "# NATURAL LANGUAGE PROCESSING\n",
    "import nltk                                    # Natural Language Toolkit: NLP library\n",
    "from nltk.corpus import stopwords              # Common stopwords (e.g., \"the\", \"a\", \"is\") to filter out\n",
    "import re                                      # Regular expressions: pattern matching for text cleaning (HTML removal, punctuation)\n",
    "\n",
    "# NLTK DATA DOWNLOAD AND INITIALIZATION\n",
    "# Download stopwords corpus\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Initialize stopwords set for English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# Set random seed for reproducibility ensuring consistent results across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# DISPLAY SETTINGS\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)      # Show all columns when printing DataFrames\n",
    "pd.set_option('display.max_colwidth', 100)      # Maximum column width when displaying text\n",
    "\n",
    "# Configure matplotlib and seaborn visualization styles\n",
    "plt.style.use('seaborn-v0_8')                   # Use seaborn style for plots\n",
    "sns.set_palette(\"husl\")                         # Set color palette for seaborn plots\n",
    "# Display plots inline in Jupyter notebook\n",
    "%matplotlib inline                              \n",
    "\n",
    "print(\"Libraries imported and environment set up successfully.\")\n",
    "print(f\"NLTK data ready. Loaded {len(stop_words)} stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Data Loading & Preprocessing\n",
    "\n",
    "#### A1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV file\n",
    "# The dataset contains movie reviews and their sentiment labels\n",
    "df = pd.read_csv('../dataset/IMDB-Dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()  # Display first few rows to inspect the data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "msing_vl = df.isnull().sum().sum()\n",
    "print(f\"Missing values: {msing_vl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Text Cleaning\n",
    "\n",
    "Removing HTML tags, Converting to lowercase, Removing punctuation & Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample review before cleaning\n",
    "print(\"Sample review of first 300 characters:\")\n",
    "print(df['review'].iloc[0][:300])\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "# This function implements the required preprocessing steps:\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle missing or empty text\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    # Step 1: Remove HTML tags using regex pattern matching\n",
    "    text = re.sub(r'<[^>]+>', '', str(text))\n",
    "    # Step 2: Convert to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    # Step 3: Remove punctuation - keep alphanumeric and spaces only\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # Step 4: Simple tokenization - split on whitespace\n",
    "    tokens = text.split()\n",
    "    # Step 5: Remove stopwords - common words like \"the\", \"a\", \"is\" that don't carry sentiment and \n",
    "    # - short words less than 2 characters to filter out noise\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    # Join tokens back to string for vectorization\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test cleaning function\n",
    "sample_text = df['review'].iloc[0]\n",
    "cleaned_sample = clean_text(sample_text)\n",
    "print(\"Original df with first 200 chars:\", sample_text[:300])\n",
    "print(\"Cleaned df with first 200 chars:\", cleaned_sample[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all reviews\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "print(\"Apply cleaning to all reviews completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical features using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "# We have chosen TF-IDF has over Bag-of-Words as it weights words by importance. rare words get higher scores\n",
    "# Parameters:\n",
    "#   - max_features=5000: Limit vocabulary to top 5000 features - this reduces dimensionality\n",
    "#   - ngram_range=(1, 2): Include unigrams (single words) and bigrams (word pairs) for context\n",
    "#   - min_df=2: Ignore terms appearing in less than 2 documents (filter rare terms)\n",
    "#   - max_df=0.95: Ignore terms appearing in more than 95% of documents (filter common terms)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_review'])  # Transform text to TF-IDF matrix\n",
    "y = df['sentiment'].map({'negative': 0, 'positive': 1})  # Convert labels to binary (0=negative, 1=positive)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Handling class balance if dataset is imbalanced\n",
    "\n",
    "Check sentiment distribution, Check if balanced, Visualize sentiment distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(f\"\\nSentiment distribution:\\n{sentiment_counts}\")\n",
    "\n",
    "percg = df['sentiment'].value_counts(normalize=True) * 100\n",
    "print(f\"\\nPercentage:\\n{percg}\")\n",
    "\n",
    "# Check if balanced\n",
    "is_balanced = df['sentiment'].value_counts(normalize=True).std() < 0.05\n",
    "print(f\"\\nDataset is {'balanced' if is_balanced else 'imbalanced'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'red'], edgecolor='black')\n",
    "plt.title('Sentiment Distribution in IMDB Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(sentiment_counts):\n",
    "    plt.text(i, v + 500, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: First Exploratory Data Analysis (EDA)\n",
    "\n",
    "B1. Most Frequent Words by Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words for positive and negative reviews\n",
    "# Part B requirement: \"Compute most frequent words in positive vs negative reviews\"\n",
    "\n",
    "def get_top_words(reviews, n=20):\n",
    "    #Get top N words from reviews by frequency count.\n",
    "    all_words = []\n",
    "    # Collect all words from all reviews in the set\n",
    "    for review in reviews:\n",
    "        all_words.extend(review.split())  # Split each review into words and add to list\n",
    "    # Count word frequencies and return top N\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "# Positive reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['cleaned_review']\n",
    "positive_words = get_top_words(positive_reviews, 20)\n",
    "\n",
    "# Negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['cleaned_review']\n",
    "negative_words = get_top_words(negative_reviews, 20)\n",
    "\n",
    "print(\"Top 10 words in POSITIVE reviews:\")\n",
    "for word, count in positive_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in NEGATIVE reviews:\")\n",
    "for word, count in negative_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Word Clouds for Each Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Positive reviews word cloud\n",
    "positive_text = ' '.join(positive_reviews)\n",
    "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "axes[0].imshow(wordcloud_positive, interpolation='bilinear')\n",
    "axes[0].set_title('Positive Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative reviews word cloud\n",
    "negative_text = ' '.join(negative_reviews)\n",
    "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
    "axes[1].imshow(wordcloud_negative, interpolation='bilinear')\n",
    "axes[1].set_title('Negative Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Histogram of Review Lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review lengths\n",
    "df['review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Create histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram by sentiment\n",
    "positive_lengths = df[df['sentiment'] == 'positive']['review_length']\n",
    "negative_lengths = df[df['sentiment'] == 'negative']['review_length']\n",
    "\n",
    "axes[0].hist([positive_lengths, negative_lengths], bins=50, alpha=0.7, \n",
    "             label=['Positive', 'Negative'], color=['green', 'red'])\n",
    "axes[0].set_xlabel('Review Length (words)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Review Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([positive_lengths, negative_lengths], tick_labels=['Positive', 'Negative'])\n",
    "axes[1].set_ylabel('Review Length (words)', fontsize=12)\n",
    "axes[1].set_title('Review Length Box Plot by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Positive: Mean={positive_lengths.mean():.1f}, Median={positive_lengths.median():.1f}, Std={positive_lengths.std():.1f}\")\n",
    "print(f\"Negative: Mean={negative_lengths.mean():.1f}, Median={negative_lengths.median():.1f}, Std={negative_lengths.std():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. Do Positive vs Negative Reviews Differ in Length or Vocabulary?\n",
    "\n",
    "**Findings:**\n",
    "1. **Length differences**: Positive and negative reviews have similar average lengths, but there may be slight variations in distribution.\n",
    "2. **Vocabulary differences**: Positive reviews contain words like \"great\", \"excellent\", \"wonderful\", \"love\", \"good\", \"best\", \"amazing\", \"enjoy\", \"perfect\", \"brilliant\".\n",
    "3. **Negative reviews** contain words like \"bad\", \"worst\", \"awful\", \"terrible\", \"horrible\", \"waste\", \"boring\", \"disappointing\", \"poor\", \"dull\".\n",
    "4. **Key insight**: While length may be similar, vocabulary clearly differs between positive and negative reviews, which makes sentiment classification feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Feature Engineering\n",
    "\n",
    "C1. Create sentiment lexicon score per review (positive − negative word counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT LEXICON SCORE - TWO APPROACHES,  MANUAL LEXICON & DYNAMIC EXTRACTION\n",
    "# (Choose ONE approach by commenting/uncommenting the appropriate sections below)\n",
    "# ============================================================================\n",
    "# \n",
    "# OPTION 1: MANUAL LEXICON\n",
    "#   - Pros: Fast, interpretable, based on linguistic knowledge\n",
    "#   - Cons: Limited coverage, may miss domain-specific terms\n",
    "#\n",
    "# OPTION 2: DYNAMIC EXTRACTION (Advanced - Data-Driven)\n",
    "#   - Pros: Discovers domain-specific words automatically, data-driven\n",
    "#   - Cons: More complex, may include noise, requires data to be loaded first\n",
    "# ============================================================================\n",
    "\n",
    "# OPTION 1: MANUAL LEXICON (Pre-defined 20 word lists)\n",
    "# Comment out this entire section if using OPTION 2\n",
    "\"\"\"\n",
    "positive_words_lexicon = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', \n",
    "                          'love', 'like', 'best', 'perfect', 'brilliant', 'awesome', 'enjoy',\n",
    "                          'beautiful', 'outstanding', 'superb', 'marvelous', 'fabulous', 'delightful','life']\n",
    "\n",
    "negative_words_lexicon = ['bad', 'worst', 'awful', 'terrible', 'horrible', 'hate', 'disappointing',\n",
    "                          'poor', 'boring', 'dull', 'waste', 'stupid', 'annoying', 'frustrating',\n",
    "                          'disgusting', 'pathetic', 'ridiculous', 'unpleasant', 'ugly','nothing']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# OPTION 2: DYNAMIC EXTRACTION (Extract from dataset)\n",
    "# Uncomment this entire section if using OPTION 2 (and comment out OPTION 1 above)\n",
    "\n",
    "# Get word frequencies for positive and negative reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['cleaned_review']\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['cleaned_review']\n",
    "\n",
    "# Count words in each class\n",
    "positive_word_counts = Counter()\n",
    "negative_word_counts = Counter()\n",
    "\n",
    "for review in positive_reviews:\n",
    "    positive_word_counts.update(review.split())\n",
    "\n",
    "for review in negative_reviews:\n",
    "    negative_word_counts.update(review.split())\n",
    "\n",
    "# Calculate relative frequency (word frequency / total words in class)\n",
    "total_positive_words = sum(positive_word_counts.values())\n",
    "total_negative_words = sum(negative_word_counts.values())\n",
    "\n",
    "# Get all unique words\n",
    "all_words = set(positive_word_counts.keys()) | set(negative_word_counts.keys())\n",
    "\n",
    "# Calculate sentiment score for each word\n",
    "# Score = (freq in positive / total positive) - (freq in negative / total negative)\n",
    "word_sentiment_scores = {}\n",
    "for word in all_words:\n",
    "    pos_freq = positive_word_counts.get(word, 0) / total_positive_words if total_positive_words > 0 else 0\n",
    "    neg_freq = negative_word_counts.get(word, 0) / total_negative_words if total_negative_words > 0 else 0\n",
    "    word_sentiment_scores[word] = pos_freq - neg_freq\n",
    "\n",
    "# Extract top positive and negative words (words with highest/lowest scores)\n",
    "# Filter by minimum frequency to avoid rare words\n",
    "min_frequency = 10  # Word must appear at least 10 times\n",
    "positive_words_dynamic = [word for word, score in sorted(word_sentiment_scores.items(), \n",
    "                                                          key=lambda x: x[1], reverse=True)\n",
    "                          if positive_word_counts.get(word, 0) >= min_frequency][:30]\n",
    "\n",
    "negative_words_dynamic = [word for word, score in sorted(word_sentiment_scores.items(), \n",
    "                                                          key=lambda x: x[1])\n",
    "                          if negative_word_counts.get(word, 0) >= min_frequency][:30]\n",
    "\n",
    "print(\"Top 20 Dynamically Extracted +ve Wrds:\", positive_words_dynamic[:20])\n",
    "print(\"Top 20 Dynamically Extracted -ve Wrds:\", negative_words_dynamic[:20])\n",
    "\n",
    "# Use dynamically extracted lexicons\n",
    "positive_words_lexicon = positive_words_dynamic\n",
    "negative_words_lexicon = negative_words_dynamic\n",
    "\n",
    "\n",
    "#Calculate sentiment lexicon score (positive - negative word counts).\n",
    "def calculate_lexicon_score(text):\n",
    "    \n",
    "    # Returns a score where:\n",
    "    #- Positive values indicate more positive words ; Negative values indicate more negative words\n",
    "    #- Zero indicates balanced or neutral\n",
    "   \n",
    "    words = text.lower().split()  # Convert to lowercase and split into words\n",
    "    # Count positive words in the text\n",
    "    positive_count = sum(1 for word in words if word in positive_words_lexicon)\n",
    "    # Count negative words in the text\n",
    "    negative_count = sum(1 for word in words if word in negative_words_lexicon)\n",
    "    # Return difference (positive - negative)\n",
    "    return positive_count - negative_count\n",
    "\n",
    "# Calculate lexicon scores\n",
    "df['lexicon_score'] = df['cleaned_review'].apply(calculate_lexicon_score)\n",
    "\n",
    "print(\"Lexicon score statistics:\")\n",
    "print(df['lexicon_score'].describe())\n",
    "print(f\"\\nMean by sentiment:\\n{df.groupby('sentiment')['lexicon_score'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Extract N-grams (Bigrams and Trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bigrams and trigrams\n",
    "# N-grams capture context and phrases that single words miss (e.g., \"not good\" vs \"good\")\n",
    "# Extract bigrams (2-word phrases)\n",
    "# Example: \"very good\", \"not bad\", \"really great\"\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=20)\n",
    "bigrams = bigram_vectorizer.fit_transform(df['cleaned_review'])\n",
    "bigram_features = bigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract trigrams (3-word phrases)\n",
    "# Example: \"not very good\", \"one of the\"\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=20)\n",
    "trigrams = trigram_vectorizer.fit_transform(df['cleaned_review'])\n",
    "trigram_features = trigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top 10 Bigrams:\", list(bigram_features[:10]))\n",
    "print(\"Top 10 Trigrams:\", list(trigram_features[:10]))\n",
    "\n",
    "# Store n-gram counts as features\n",
    "df['bigram_count'] = [bigrams[i].sum() for i in range(len(df))]\n",
    "df['trigram_count'] = [trigrams[i].sum() for i in range(len(df))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3. Compute Readability Metrics\n",
    "Compute readability metrics (e.g., average word length, sentence length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #These metrics capture writing style and complexity, which may correlate with sentiment expression\n",
    "    #Compute readability metrics: average word length, sentence length.\n",
    "    \n",
    "    #Returns: avg_sentence_length: Average number of words per sentence; avg_sentence_length: Average number of words per sentence\n",
    "def compute_readability_metrics(text):\n",
    "    # Calculate average word length the indicator of vocabulary complexity\n",
    "    words = text.split()\n",
    "    if len(words) > 0:\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        avg_word_length = 0\n",
    "    \n",
    "    # Calculate average sentence length the indicator of writing complexity\n",
    "    # Approximate sentence count by counting sentence-ending punctuation\n",
    "    # Note: This is an approximation since cleaned text may have removed some punctuation\n",
    "    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n",
    "    avg_sentence_length = len(words) / sentence_count if sentence_count > 0 else len(words)\n",
    "    \n",
    "    return avg_word_length, avg_sentence_length\n",
    "\n",
    "# Apply readability metrics\n",
    "readability_metrics = df['cleaned_review'].apply(compute_readability_metrics)\n",
    "df['avg_word_length'] = [m[0] for m in readability_metrics]\n",
    "df['avg_sentence_length'] = [m[1] for m in readability_metrics]\n",
    "\n",
    "print(\"Readability metrics statistics:\")\n",
    "print(df[['avg_word_length', 'avg_sentence_length']].describe())\n",
    "print(f\"\\nMean by sentiment:\\n{df.groupby('sentiment')[['avg_word_length', 'avg_sentence_length']].mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4. Why These Engineered Features Help Sentiment Classification\n",
    "\n",
    "Importance of these Features:\n",
    "1. *Sentiment Lexicon Score*: Directly captures sentiment polarity by counting positive vs negative words. Higher scores indicate positive sentiment. Lexicon scores provide direct sentiment signals\n",
    "2. *N-grams (Bigrams/Trigrams)*: Capture context and phrases that single words miss (e.g., \"not good\" vs \"good\"). It handles negations better than unigrams\n",
    "3. *Readability Metrics*: Captures stylistic differences that may correlate with sentiment\n",
    "   Average word length: Longer words may indicate more formal or detailed reviews\n",
    "   Average sentence length: Can indicate review complexity and writing style\n",
    "   These metrics may correlate with review quality and sentiment expression\n",
    "\n",
    "Combining these with TF-IDF features provides richer feature representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second EDA & Statistical Inference\n",
    "\n",
    "D1. Hypothesis Test: Are Reviews with Higher Lexicon Scores More Likely to be Positive?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis test: Are reviews with higher lexicon scores more likely to be positive?\n",
    "# Null Hypothesis (H0): No difference in lexicon scores between positive and negative reviews\n",
    "# Alternative Hypothesis (H1): Positive reviews have higher lexicon scores than negative reviews\n",
    "# Significance level: α = 0.05\n",
    "\n",
    "# Extract lexicon scores for each sentiment class\n",
    "positive_scores = df[df['sentiment'] == 'positive']['lexicon_score']\n",
    "negative_scores = df[df['sentiment'] == 'negative']['lexicon_score']\n",
    "\n",
    "# Perform independent samples t-test (one-tailed, right-tailed)\n",
    "# Tests if positive reviews have significantly higher lexicon scores\n",
    "t_stat, p_value = ttest_ind(positive_scores, negative_scores, alternative='greater')\n",
    "\n",
    "print(f\"H0: No difference in lexicon scores\")\n",
    "print(f\"H1: Positive reviews have higher lexicon scores\")\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}\")\n",
    "print(f\"Conclusion: {'Reject H0' if p_value < 0.05 else 'Fail to reject H0'} (α = 0.05)\")\n",
    "print(f\"Reviews with higher lexicon scores are {'more likely' if p_value < 0.05 else 'not significantly more likely'} to be positive.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2. Confidence Interval for Average Review Length by Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for average review length\n",
    "# Confidence intervals provide a range estimate for the true population mean\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    \n",
    "    #Calculate confidence interval for mean using t-distribution.\n",
    "    #Args: 'data: Array-like data'; 'confidence: Confidence level (default 0.95 for 95% CI)'\n",
    "    #Returns: Tuple of (lower_bound, upper_bound)\n",
    "    n = len(data)\n",
    "    mean = data.mean()  # Sample mean\n",
    "    std_err = stats.sem(data)  # Standard error of the mean\n",
    "    # Calculate margin of error using t-distribution (for sample size < 30 or unknown population std)\n",
    "    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean - h, mean + h  # Return (lower, upper) bounds\n",
    "\n",
    "# Positive reviews\n",
    "pos_mean = positive_lengths.mean()\n",
    "pos_ci = confidence_interval(positive_lengths)\n",
    "\n",
    "# Negative reviews\n",
    "neg_mean = negative_lengths.mean()\n",
    "neg_ci = confidence_interval(negative_lengths)\n",
    "\n",
    "print(\"95% Confidence Intervals for Average Review Length:\")\n",
    "print(f\"Positive: Mean={pos_mean:.2f}, CI=[{pos_ci[0]:.2f}, {pos_ci[1]:.2f}]\")\n",
    "print(f\"Negative: Mean={neg_mean:.2f}, CI=[{neg_ci[0]:.2f}, {neg_ci[1]:.2f}]\")\n",
    "print(f\"Difference: {pos_mean - neg_mean:.2f} words, CI overlap: {'Yes' if pos_ci[1] >= neg_ci[0] and neg_ci[1] >= pos_ci[0] else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3. Visualize Sentiment Differences in Engineered Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize engineered features by sentiment\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Lexicon score distribution\n",
    "axes[0, 0].hist([df[df['sentiment'] == 'positive']['lexicon_score'],\n",
    "                 df[df['sentiment'] == 'negative']['lexicon_score']],\n",
    "                bins=30, alpha=0.7, label=['Positive', 'Negative'], color=['green', 'red'])\n",
    "axes[0, 0].set_xlabel('Lexicon Score', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Lexicon Score Distribution by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Average word length\n",
    "axes[0, 1].boxplot([df[df['sentiment'] == 'positive']['avg_word_length'],\n",
    "                    df[df['sentiment'] == 'negative']['avg_word_length']],\n",
    "                   tick_labels=['Positive', 'Negative'])\n",
    "axes[0, 1].set_ylabel('Average Word Length', fontsize=12)\n",
    "axes[0, 1].set_title('Average Word Length by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Average sentence length\n",
    "axes[1, 0].boxplot([df[df['sentiment'] == 'positive']['avg_sentence_length'],\n",
    "                    df[df['sentiment'] == 'negative']['avg_sentence_length']],\n",
    "                   tick_labels=['Positive', 'Negative'])\n",
    "axes[1, 0].set_ylabel('Average Sentence Length', fontsize=12)\n",
    "axes[1, 0].set_title('Average Sentence Length by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Review length\n",
    "axes[1, 1].boxplot([positive_lengths, negative_lengths], tick_labels=['Positive', 'Negative'])\n",
    "axes[1, 1].set_ylabel('Review Length (words)', fontsize=12)\n",
    "axes[1, 1].set_title('Review Length by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D4. Interpretation of Findings in Context\n",
    "\n",
    "1. **Lexicon Score**: Positive reviews have significantly higher lexicon scores than negative reviews as confirmed by hypothesis test. This validates that lexicon-based features are effective for sentiment classification.\n",
    "2. **Review Length**: Positive and negative reviews have similar average lengths, with overlapping confidence intervals. Length alone is not a strong predictor of sentiment.\n",
    "3. **Readability Metrics**: Average word length and sentence length show similar distributions for both sentiments, suggesting that writing style (complexity) is not strongly correlated with sentiment.\n",
    "4. **Implications**: \n",
    "   - Lexicon-based features are the most discriminative engineered features\n",
    "   - TF-IDF features capturing vocabulary differences are likely more important than readability metrics\n",
    "   - Combining lexicon scores with TF-IDF features should improve classification performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Simple Machine Learning \n",
    "\n",
    "E1. Split Data into Train/Test Sets (80/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test sets (80/20 split)\n",
    "# \n",
    "# Parameters:\n",
    "#   - test_size=0.2: 20% of data for testing, 80% for training\n",
    "#   - random_state=42: Set seed for reproducibility\n",
    "#   - stratify=y: Ensure balanced class distribution in both train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "print(f\"Train distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"Test distribution: {pd.Series(y_test).value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2. Train Models: Naïve Bayes and Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the models: Naïve Bayes, Logistic Regression\n",
    "# \n",
    "# Train Naive Bayes\n",
    "# MultinomialNB is well-suited for text classification with count/TF-IDF features\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_pred = nb_model.predict(X_test)  # Class predictions\n",
    "nb_pred_proba = nb_model.predict_proba(X_test)[:, 1]  # Probability of positive class for ROC-AUC\n",
    "\n",
    "# Train Logistic Regression\n",
    "# Logistic Regression provides interpretable coefficients and good performance for text classification\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)  # max_iter increased for convergence\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)  # Class predictions\n",
    "lr_pred_proba = lr_model.predict_proba(X_test)[:, 1]  # Probability of positive class for ROC-AUC\n",
    "\n",
    "print(f\"Model Training Completed...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3. Evaluation of Models: Confusion matrix, Accuracy, Precision, Recall, F1, ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Models: Confusion matrix, Accuracy, Precision, Recall, F1, ROC-AUC\n",
    " \n",
    "# All metrics use 'macro' averaging to handle class balance concerns (as specified in requirements)\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
    "    \n",
    "    #Evaluate model and return comprehensive metrics.\n",
    "    #Args: \n",
    "    # - y_true: True labels\n",
    "    # - y_pred: Predicted labels\n",
    "    # - y_pred_proba: Predicted probabilities for ROC-AUC; \n",
    "    # - model_name: Name of the model\n",
    "    #Returns: Dictionary containing all evaluation metrics\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)  # Overall accuracy\n",
    "    precision = precision_score(y_true, y_pred, average='macro')  # Macro-averaged precision\n",
    "    recall = recall_score(y_true, y_pred, average='macro')  # Macro-averaged recall\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')  # Macro-averaged F1 score\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)  # Area under ROC curve\n",
    "    cm = confusion_matrix(y_true, y_pred)  # Confusion matrix for visualization\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluate models\n",
    "nb_metrics = evaluate_model(y_test, nb_pred, nb_pred_proba, 'Naive Bayes')\n",
    "lr_metrics = evaluate_model(y_test, lr_pred, lr_pred_proba, 'Logistic Regression')\n",
    "\n",
    "# Print results\n",
    "print(\"Naive Bayes - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}, ROC-AUC: {:.4f}\".format(\n",
    "    nb_metrics['accuracy'], nb_metrics['precision'], nb_metrics['recall'], \n",
    "    nb_metrics['f1'], nb_metrics['roc_auc']))\n",
    "print(f\"Confusion Matrix:\\n{nb_metrics['confusion_matrix']}\")\n",
    "\n",
    "print(\"\\nLogistic Regression - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1: {:.4f}, ROC-AUC: {:.4f}\".format(\n",
    "    lr_metrics['accuracy'], lr_metrics['precision'], lr_metrics['recall'], \n",
    "    lr_metrics['f1'], lr_metrics['roc_auc']))\n",
    "print(f\"Confusion Matrix:\\n{lr_metrics['confusion_matrix']}\")\n",
    "\n",
    "# Visualize confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(nb_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Naive Bayes Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(lr_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Logistic Regression Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E4. Computing mean ± std for all metrics across cross-validation folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cross-validation folds and report mean ± std for every metric\n",
    "# Cross-validation provides more robust performance estimates by testing on multiple train/test splits\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)  # 5 folds, shuffled for randomness\n",
    "\n",
    "def cross_validate_model(model, X, y, cv):\n",
    "    \n",
    "    #This function computes all required metrics; Accuracy, Precision, Recall, F1, ROC-AUC.\n",
    "    #across all folds and returns mean ± standard deviation for each metric.\n",
    "    \n",
    "    scores = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'roc_auc': []\n",
    "    }\n",
    "    \n",
    "    # Convert y to numpy array if it's a Series\n",
    "    if isinstance(y, pd.Series):\n",
    "        y_array = y.values\n",
    "    else:\n",
    "        y_array = y\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "        y_train_cv, y_val_cv = y_array[train_idx], y_array[val_idx]\n",
    "        \n",
    "        # Create a fresh model instance for each fold\n",
    "        from sklearn.base import clone\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = model_clone.predict(X_val_cv)\n",
    "        \n",
    "        # Handle models with/without predict_proba\n",
    "        try:\n",
    "            y_pred_proba = model_clone.predict_proba(X_val_cv)[:, 1]\n",
    "        except AttributeError:\n",
    "            # For models without predict_proba e.g., LinearSVC, use decision_function\n",
    "            try:\n",
    "                decision_scores = model_clone.decision_function(X_val_cv)\n",
    "                # Normalize to [0, 1] for ROC-AUC\n",
    "                scaler = MinMaxScaler()\n",
    "                y_pred_proba = scaler.fit_transform(decision_scores.reshape(-1, 1)).ravel()\n",
    "            except:\n",
    "                # Fallback: use predict as probability\n",
    "                y_pred_proba = y_pred.astype(float)\n",
    "        \n",
    "        scores['accuracy'].append(accuracy_score(y_val_cv, y_pred))\n",
    "        scores['precision'].append(precision_score(y_val_cv, y_pred, average='macro', zero_division=0))\n",
    "        scores['recall'].append(recall_score(y_val_cv, y_pred, average='macro', zero_division=0))\n",
    "        scores['f1'].append(f1_score(y_val_cv, y_pred, average='macro', zero_division=0))\n",
    "        scores['roc_auc'].append(roc_auc_score(y_val_cv, y_pred_proba))\n",
    "    \n",
    "    return {metric: (np.mean(values), np.std(values)) for metric, values in scores.items()}\n",
    "\n",
    "# Cross-validate models\n",
    "nb_cv_scores = cross_validate_model(MultinomialNB(), X_tfidf, y, kfold)\n",
    "lr_cv_scores = cross_validate_model(LogisticRegression(max_iter=1000, random_state=42), X_tfidf, y, kfold)\n",
    "\n",
    "# Print results\n",
    "print(\"Naive Bayes - 5-Fold CV (Mean ± Std):\")\n",
    "for metric, (mean, std) in nb_cv_scores.items():\n",
    "    print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "print(\"\\nLogistic Regression - 5-Fold CV (Mean ± Std):\")\n",
    "for metric, (mean, std) in lr_cv_scores.items():\n",
    "    print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E5. Ensemble Model: Linear SVM + Gradient Boosting on TF-IDF features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model\n",
    "# Include ensemble model: Linear SVM + Gradient Boosting on TF-IDF features\n",
    "# Ensemble models combine multiple algorithms to improve performance and robustness\n",
    "svm_model = LinearSVC(max_iter=1000, random_state=42)  # Linear SVM: fast and effective for text\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)  # Gradient Boosting: powerful ensemble method\n",
    "\n",
    "# Train ensemble models\n",
    "svm_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "\n",
    "# For SVM, use decision function to get probabilities (approximate)\n",
    "svm_calibrated = CalibratedClassifierCV(svm_model, method='sigmoid', cv=3)\n",
    "svm_calibrated.fit(X_train, y_train)\n",
    "svm_pred_proba = svm_calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "gb_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate ensemble models\n",
    "svm_metrics = evaluate_model(y_test, svm_pred, svm_pred_proba, 'Linear SVM')\n",
    "gb_metrics = evaluate_model(y_test, gb_pred, gb_pred_proba, 'Gradient Boosting')\n",
    "\n",
    "# Cross-validate ensemble models\n",
    "# For SVM, use CalibratedClassifierCV to get probabilities\n",
    "svm_base = LinearSVC(max_iter=1000, random_state=42)\n",
    "svm_calibrated = CalibratedClassifierCV(svm_base, method='sigmoid', cv=3)\n",
    "svm_cv_scores = cross_validate_model(svm_calibrated, X_tfidf, y, kfold)\n",
    "\n",
    "gb_cv_scores = cross_validate_model(GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5), X_tfidf, y, kfold)\n",
    "\n",
    "print(\"Linear SVM - 5-Fold CV (Mean ± Std):\")\n",
    "for metric, (mean, std) in svm_cv_scores.items():\n",
    "    print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "print(\"\\nGradient Boosting - 5-Fold CV (Mean ± Std):\")\n",
    "for metric, (mean, std) in gb_cv_scores.items():\n",
    "    print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E6. Feature Importance and Coefficient Interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Logistic Regression\n",
    "# Part E requirement: \"Perform feature importance or coefficient interpretation to identify key sentiment indicators\"\n",
    "# Logistic Regression coefficients indicate which words/phrases are strong predictors of sentiment\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()  # Get feature names (words/phrases)\n",
    "lr_coef = lr_model.coef_[0]  # Get coefficients (positive = positive sentiment indicator)\n",
    "\n",
    "# Get top positive and negative features (words/phrases)\n",
    "# Positive coefficients indicate positive sentiment predictors\n",
    "# Negative coefficients indicate negative sentiment predictors\n",
    "top_positive_features = np.argsort(lr_coef)[-10:][::-1]  # Top 10 positive indicators\n",
    "top_negative_features = np.argsort(lr_coef)[:10]  # Top 10 negative indicators\n",
    "\n",
    "print(\"Top 10 Positive Indicators:\", [feature_names[idx] for idx in top_positive_features])\n",
    "print(\"Top 10 Negative Indicators:\", [feature_names[idx] for idx in top_negative_features])\n",
    "\n",
    "# Get feature importance from Gradient Boosting\n",
    "gb_feature_importance = gb_model.feature_importances_\n",
    "top_gb_features = np.argsort(gb_feature_importance)[-10:][::-1]\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Gradient Boosting):\", [feature_names[idx] for idx in top_gb_features])\n",
    "\n",
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Logistic Regression coefficients\n",
    "top_pos_words = [feature_names[idx] for idx in top_positive_features[:10]]\n",
    "top_pos_coefs = [lr_coef[idx] for idx in top_positive_features[:10]]\n",
    "top_neg_words = [feature_names[idx] for idx in top_negative_features[:10]]\n",
    "top_neg_coefs = [lr_coef[idx] for idx in top_negative_features[:10]]\n",
    "\n",
    "axes[0].barh(range(len(top_pos_words)), top_pos_coefs, color='green', alpha=0.7, label='Positive')\n",
    "axes[0].barh(range(len(top_neg_words), len(top_pos_words) + len(top_neg_words)), \n",
    "             [abs(c) for c in top_neg_coefs], color='red', alpha=0.7, label='Negative')\n",
    "axes[0].set_yticks(range(len(top_pos_words) + len(top_neg_words)))\n",
    "axes[0].set_yticklabels(top_pos_words + top_neg_words)\n",
    "axes[0].set_xlabel('Coefficient Value', fontsize=12)\n",
    "axes[0].set_title('Top Sentiment Indicators (Logistic Regression)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Gradient Boosting feature importance\n",
    "top_gb_words = [feature_names[idx] for idx in top_gb_features[:20]]\n",
    "top_gb_importance = [gb_feature_importance[idx] for idx in top_gb_features[:20]]\n",
    "\n",
    "axes[1].barh(range(len(top_gb_words)), top_gb_importance, color='blue', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(top_gb_words)))\n",
    "axes[1].set_yticklabels(top_gb_words)\n",
    "axes[1].set_xlabel('Feature Importance', fontsize=12)\n",
    "axes[1].set_title('Top Features (Gradient Boosting)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E7. Performance Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary table\n",
    "results_summary = {\n",
    "    'Model': ['Naive Bayes', 'Logistic Regression', 'Linear SVM', 'Gradient Boosting'],\n",
    "    'Accuracy': [\n",
    "        f\"{nb_cv_scores['accuracy'][0]:.4f} ± {nb_cv_scores['accuracy'][1]:.4f}\",\n",
    "        f\"{lr_cv_scores['accuracy'][0]:.4f} ± {lr_cv_scores['accuracy'][1]:.4f}\",\n",
    "        f\"{svm_cv_scores['accuracy'][0]:.4f} ± {svm_cv_scores['accuracy'][1]:.4f}\",\n",
    "        f\"{gb_cv_scores['accuracy'][0]:.4f} ± {gb_cv_scores['accuracy'][1]:.4f}\"\n",
    "    ],\n",
    "    'Precision (Macro)': [\n",
    "        f\"{nb_cv_scores['precision'][0]:.4f} ± {nb_cv_scores['precision'][1]:.4f}\",\n",
    "        f\"{lr_cv_scores['precision'][0]:.4f} ± {lr_cv_scores['precision'][1]:.4f}\",\n",
    "        f\"{svm_cv_scores['precision'][0]:.4f} ± {svm_cv_scores['precision'][1]:.4f}\",\n",
    "        f\"{gb_cv_scores['precision'][0]:.4f} ± {gb_cv_scores['precision'][1]:.4f}\"\n",
    "    ],\n",
    "    'Recall (Macro)': [\n",
    "        f\"{nb_cv_scores['recall'][0]:.4f} ± {nb_cv_scores['recall'][1]:.4f}\",\n",
    "        f\"{lr_cv_scores['recall'][0]:.4f} ± {lr_cv_scores['recall'][1]:.4f}\",\n",
    "        f\"{svm_cv_scores['recall'][0]:.4f} ± {svm_cv_scores['recall'][1]:.4f}\",\n",
    "        f\"{gb_cv_scores['recall'][0]:.4f} ± {gb_cv_scores['recall'][1]:.4f}\"\n",
    "    ],\n",
    "    'F1 (Macro)': [\n",
    "        f\"{nb_cv_scores['f1'][0]:.4f} ± {nb_cv_scores['f1'][1]:.4f}\",\n",
    "        f\"{lr_cv_scores['f1'][0]:.4f} ± {lr_cv_scores['f1'][1]:.4f}\",\n",
    "        f\"{svm_cv_scores['f1'][0]:.4f} ± {svm_cv_scores['f1'][1]:.4f}\",\n",
    "        f\"{gb_cv_scores['f1'][0]:.4f} ± {gb_cv_scores['f1'][1]:.4f}\"\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        f\"{nb_cv_scores['roc_auc'][0]:.4f} ± {nb_cv_scores['roc_auc'][1]:.4f}\",\n",
    "        f\"{lr_cv_scores['roc_auc'][0]:.4f} ± {lr_cv_scores['roc_auc'][1]:.4f}\",\n",
    "        f\"{svm_cv_scores['roc_auc'][0]:.4f} ± {svm_cv_scores['roc_auc'][1]:.4f}\",\n",
    "        f\"{gb_cv_scores['roc_auc'][0]:.4f} ± {gb_cv_scores['roc_auc'][1]:.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"Model Performance Summary (5-Fold CV: Mean ± Std)\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Presentation & Reflection\n",
    "\n",
    "F1. Summarizing the Insights from Text Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Key Insights:*\n",
    "\n",
    "1. *Vocabulary Differences*: Positive and negative reviews have distinct vocabularies. Positive reviews frequently use words like \"great\", \"excellent\", \"wonderful\", \"love\", while negative reviews use \"bad\", \"worst\", \"awful\", \"terrible\", \"horrible\".\n",
    "\n",
    "2. *Review Length*: Positive and negative reviews have similar average lengths, suggesting that sentiment is not determined by review length but by the choice of words and phrases.\n",
    "\n",
    "3. *Lexicon Scores*: Reviews with higher lexicon scores (more positive words) are significantly more likely to be positive, validating the effectiveness of lexicon-based features.\n",
    "\n",
    "4. *Feature Importance*: Logistic Regression coefficients and Gradient Boosting feature importance reveal that specific words and phrases (especially n-grams) are strong predictors of sentiment.\n",
    "\n",
    "5. *Model Performance*: All models (Naive Bayes, Logistic Regression, Linear SVM, Gradient Boosting) achieve high performance (>85% accuracy), with Gradient Boosting and Logistic Regression performing best.\n",
    "\n",
    "6. *Context Matters*: N-grams (bigrams and trigrams) capture context that single words miss, which is crucial for handling negation and idioms.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
