{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Review Sentiment Analysis with EDA, Feature Engineering, and ML\n",
    "\n",
    "This notebook implements a comprehensive sentiment analysis pipeline covering:\n",
    "- Data Loading & Preprocessing, First EDA, Feature Engineering, Second EDA & Statistical Inference, Machine Learning, Presentation & Reflection\n",
    "\n",
    "### Dataset\n",
    "- **Source**: IMDB Movie Reviews Dataset - https://www.kaggle.com/datasets/mahmoudshaheen1134/imdp-data\n",
    "- **Location**: `../dataset/IMDB-Dataset.csv`\n",
    "### Authors\n",
    "- **Students**: Ainedembe Denis, Musinguzi Benson\n",
    "- **Lecturer**: Harriet Sibitenda (PhD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and environment set up successfully.\n",
      "NLTK data ready. Loaded 198 stopwords.\n"
     ]
    }
   ],
   "source": [
    "# DATA MANIPULATION AND ANALYSIS\n",
    "import pandas as pd  # Data manipulation and analysis: DataFrames, data loading, cleaning\n",
    "import numpy as np   # Numerical computing: arrays, mathematical operations, random number generation\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt   # Plotting library: create charts, graphs, histograms\n",
    "import seaborn as sns             # Statistical visualization: enhanced plots, heatmaps, statistical graphics\n",
    "from wordcloud import WordCloud   # Word cloud generation: visualize most frequent words in text\n",
    "\n",
    "# MACHINE LEARNING - MODEL SELECTION AND TRAINING\n",
    "from sklearn.model_selection import train_test_split  # Split data into training and testing sets\n",
    "from sklearn.model_selection import KFold             # K-fold cross-validation iterator\n",
    "\n",
    "# MACHINE LEARNING - FEATURE EXTRACTION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text to TF-IDF features (term frequency-inverse document frequency)\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Convert text to bag-of-words count features\n",
    "\n",
    "# MACHINE LEARNING - CLASSIFICATION MODELS\n",
    "from sklearn.naive_bayes import MultinomialNB               # Naive Bayes classifier for text classification\n",
    "from sklearn.linear_model import LogisticRegression         # Logistic regression classifier\n",
    "from sklearn.svm import LinearSVC                           # Linear Support Vector Machine classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier     # Gradient Boosting ensemble classifier\n",
    "\n",
    "# MACHINE LEARNING - MODEL EVALUATION METRICS\n",
    "from sklearn.metrics import accuracy_score      # Calculate classification accuracy\n",
    "from sklearn.metrics import precision_score     # Calculate precision - positive predictive value\n",
    "from sklearn.metrics import recall_score        # Calculate recall (sensitivity, true positive rate)\n",
    "from sklearn.metrics import f1_score            # Calculate F1 score (harmonic mean of precision and recall)\n",
    "from sklearn.metrics import confusion_matrix    # Generate confusion matrix for classification results\n",
    "from sklearn.metrics import roc_auc_score       # Calculate ROC-AUC (Area Under ROC Curve)\n",
    "\n",
    "# MACHINE LEARNING - UTILITIES\n",
    "from sklearn.utils import resample                      # Resample datasets for balancing classes\n",
    "from sklearn.preprocessing import MinMaxScaler          # Scale features to a specific range (0-1)\n",
    "from sklearn.base import clone                          # Clone/copy machine learning models\n",
    "from sklearn.calibration import CalibratedClassifierCV  # Calibrate classifier probabilities\n",
    "\n",
    "# STATISTICAL ANALYSIS\n",
    "from scipy import stats               # Statistical functions: distributions, statistical tests (stats.sem, stats.t.ppf)\n",
    "from scipy.stats import ttest_ind     # Independent samples t-test for hypothesis testing\n",
    "\n",
    "# NATURAL LANGUAGE PROCESSING\n",
    "import nltk                                    # Natural Language Toolkit: NLP library\n",
    "from nltk.corpus import stopwords              # Common stopwords (e.g., \"the\", \"a\", \"is\") to filter out\n",
    "import re                                      # Regular expressions: pattern matching for text cleaning (HTML removal, punctuation)\n",
    "\n",
    "# NLTK DATA DOWNLOAD AND INITIALIZATION\n",
    "# Download stopwords corpus\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Initialize stopwords set for English language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# Set random seed for reproducibility (ensures consistent results across runs)\n",
    "np.random.seed(42)\n",
    "\n",
    "# DISPLAY SETTINGS\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)      # Show all columns when printing DataFrames\n",
    "pd.set_option('display.max_colwidth', 100)      # Maximum column width when displaying text\n",
    "\n",
    "# Configure matplotlib and seaborn visualization styles\n",
    "plt.style.use('seaborn-v0_8')                   # Use seaborn style for plots\n",
    "sns.set_palette(\"husl\")                         # Set color palette for seaborn plots\n",
    "# Display plots inline in Jupyter notebook\n",
    "%matplotlib inline                              \n",
    "\n",
    "print(\"Libraries imported and environment set up successfully.\")\n",
    "print(f\"NLTK data ready. Loaded {len(stop_words)} stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Data Loading & Preprocessing\n",
    "\n",
    "#### A1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked....</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei off...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked....   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-ti...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei off...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from CSV file\n",
    "# The dataset contains movie reviews and their sentiment labels\n",
    "df = pd.read_csv('../dataset/IMDB-Dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()  # Display first few rows to inspect the data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "msing_vl = df.isnull().sum().sum()\n",
    "print(f\"Missing values: {msing_vl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Text Cleaning\n",
    "\n",
    "Removing HTML tags, Converting to lowercase, Removing punctuation & Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample review before cleaning\n",
    "print(\"Sample review of first 300 characters:\")\n",
    "print(df['review'].iloc[0][:300])\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "# This function implements the required preprocessing steps:\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle missing or empty text\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    # Step 1: Remove HTML tags using regex pattern matching\n",
    "    text = re.sub(r'<[^>]+>', '', str(text))\n",
    "    # Step 2: Convert to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    # Step 3: Remove punctuation - keep alphanumeric and spaces only\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # Step 4: Simple tokenization - split on whitespace\n",
    "    tokens = text.split()\n",
    "    # Step 5: Remove stopwords - common words like \"the\", \"a\", \"is\" that don't carry sentiment and \n",
    "    # - short words less than 2 characters to filter out noise\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    # Join tokens back to string for vectorization\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test cleaning function\n",
    "sample_text = df['review'].iloc[0]\n",
    "cleaned_sample = clean_text(sample_text)\n",
    "print(\"Original df with first 200 chars:\", sample_text[:300])\n",
    "print(\"Cleaned df with first 200 chars:\", cleaned_sample[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all reviews\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "print(\"Apply cleaning to all reviews completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical features using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "# TF-IDF is chosen over Bag-of-Words as it weights words by importance (rare words get higher scores)\n",
    "# Parameters:\n",
    "#   - max_features=5000: Limit vocabulary to top 5000 features (reduces dimensionality)\n",
    "#   - ngram_range=(1, 2): Include unigrams (single words) and bigrams (word pairs) for context\n",
    "#   - min_df=2: Ignore terms appearing in less than 2 documents (filter rare terms)\n",
    "#   - max_df=0.95: Ignore terms appearing in more than 95% of documents (filter common terms)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_review'])  # Transform text to TF-IDF matrix\n",
    "y = df['sentiment'].map({'negative': 0, 'positive': 1})  # Convert labels to binary (0=negative, 1=positive)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Label distribution: {y.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Handling class balance if dataset is imbalanced\n",
    "\n",
    "Check sentiment distribution, Check if balanced, Visualize sentiment distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(f\"\\nSentiment distribution:\\n{sentiment_counts}\")\n",
    "print(f\"\\nPercentage:\\n{df['sentiment'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# Check if balanced\n",
    "is_balanced = df['sentiment'].value_counts(normalize=True).std() < 0.05\n",
    "print(f\"\\nDataset is {'balanced' if is_balanced else 'imbalanced'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'red'], edgecolor='black')\n",
    "plt.title('Sentiment Distribution in IMDB Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(sentiment_counts):\n",
    "    plt.text(i, v + 500, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: First Exploratory Data Analysis (EDA)\n",
    "\n",
    "B1. Most Frequent Words by Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words for positive and negative reviews\n",
    "# Part B requirement: \"Compute most frequent words in positive vs negative reviews\"\n",
    "from collections import Counter\n",
    "\n",
    "def get_top_words(reviews, n=20):\n",
    "    #Get top N words from reviews by frequency count.\n",
    "    all_words = []\n",
    "    # Collect all words from all reviews in the set\n",
    "    for review in reviews:\n",
    "        all_words.extend(review.split())  # Split each review into words and add to list\n",
    "    # Count word frequencies and return top N\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "# Positive reviews\n",
    "positive_reviews = df[df['sentiment'] == 'positive']['cleaned_review']\n",
    "positive_words = get_top_words(positive_reviews, 20)\n",
    "\n",
    "# Negative reviews\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['cleaned_review']\n",
    "negative_words = get_top_words(negative_reviews, 20)\n",
    "\n",
    "print(\"Top 10 words in POSITIVE reviews:\")\n",
    "for word, count in positive_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in NEGATIVE reviews:\")\n",
    "for word, count in negative_words[:10]:\n",
    "    print(f\"  {word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Word Clouds for Each Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Positive reviews word cloud\n",
    "positive_text = ' '.join(positive_reviews)\n",
    "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "axes[0].imshow(wordcloud_positive, interpolation='bilinear')\n",
    "axes[0].set_title('Positive Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative reviews word cloud\n",
    "negative_text = ' '.join(negative_reviews)\n",
    "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
    "axes[1].imshow(wordcloud_negative, interpolation='bilinear')\n",
    "axes[1].set_title('Negative Reviews Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Histogram of Review Lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review lengths\n",
    "df['review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Create histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram by sentiment\n",
    "positive_lengths = df[df['sentiment'] == 'positive']['review_length']\n",
    "negative_lengths = df[df['sentiment'] == 'negative']['review_length']\n",
    "\n",
    "axes[0].hist([positive_lengths, negative_lengths], bins=50, alpha=0.7, \n",
    "             label=['Positive', 'Negative'], color=['green', 'red'])\n",
    "axes[0].set_xlabel('Review Length (words)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Review Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([positive_lengths, negative_lengths], labels=['Positive', 'Negative'])\n",
    "axes[1].set_ylabel('Review Length (words)', fontsize=12)\n",
    "axes[1].set_title('Review Length Box Plot by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"Positive: Mean={positive_lengths.mean():.1f}, Median={positive_lengths.median():.1f}, Std={positive_lengths.std():.1f}\")\n",
    "print(f\"Negative: Mean={negative_lengths.mean():.1f}, Median={negative_lengths.median():.1f}, Std={negative_lengths.std():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. Do Positive vs Negative Reviews Differ in Length or Vocabulary?\n",
    "\n",
    "**Findings:**\n",
    "1. **Length differences**: Positive and negative reviews have similar average lengths, but there may be slight variations in distribution.\n",
    "2. **Vocabulary differences**: Positive reviews contain words like \"great\", \"excellent\", \"wonderful\", \"love\", \"good\", \"best\", \"amazing\", \"enjoy\", \"perfect\", \"brilliant\".\n",
    "3. **Negative reviews** contain words like \"bad\", \"worst\", \"awful\", \"terrible\", \"horrible\", \"waste\", \"boring\", \"disappointing\", \"poor\", \"dull\".\n",
    "4. **Key insight**: While length may be similar, vocabulary clearly differs between positive and negative reviews, which makes sentiment classification feasible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
